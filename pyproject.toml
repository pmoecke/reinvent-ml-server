# [build-system]
# requires = ["uv_build >= 0.8.12, <0.9.0"]
# build-backend = "uv_build"

[project]
name = "reinvent-ml-service"
version = "0.1.0"
description = "Host for SceneScript and other ML services for the ReInvent web app"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "fastapi>=0.118.2",
    "uvicorn>=0.37.0",
    "llama-index",
    "llama-index-embeddings-huggingface",
    "llama-index-llms-huggingface-api",
    "llama-index-llms-openai-like",
    "llama-index-readers-docling<0.4.1",
    "llama-index-node-parser-docling<0.4.1",
    "llama-index-vector-stores-postgres",
    "psycopg[binary]>=3.2.10",
    "python-dotenv>=1.1.1",
    "jupyter>=1.0.0",
    "plotly>=5.18.0",
    "pandas>=2.0.0",
    "scipy>=1.10.0",
    "projectaria-tools>=1.0.0",
    "einops>=0.7.0",
    "omegaconf>=2.3.0",
    "torch==2.4.0",
    "torchvision",  # Will automatically match compatible version for torch 2.4
    "torchsparse @ git+https://github.com/mit-han-lab/torchsparse.git",
    "uvicorn>=0.37.0",
]

[tool.uv]
package = false

[tool.uv.extra-build-dependencies]
# We force torch and numpy into the build environment so setup.py doesn't crash
torchsparse = ["torch==2.4.0", "numpy==1.24.2"]

# Tell uv to look for CUDA 12.1 wheels specifically
[[tool.uv.index]]
name = "pytorch"
url = "https://download.pytorch.org/whl/cu121"
explicit = true  # Force torch to come from here, not PyPI

[tool.uv.sources]
torch = { index = "pytorch" }
torchvision = { index = "pytorch" }

[tool.uv.build-backend]
module-name = "ml_service"

[project.scripts]
start-server = "ml_service.app:start_server"

[tool.ruff]
target-version = "py313"

[tool.ruff.lint]
# Flake-8 Bugbear: "B" disabled because it flagged "Depends" calls from fastapi
select = ["E", "F", "W", "I", "SIM"] 
